{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "91e227c4",
      "metadata": {
        "id": "91e227c4"
      },
      "source": [
        "###  1-Use the datasets library from Hugging Face to download the arabic- generated-abstracts dataset directly into a Python environment (By Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792b9e4b",
      "metadata": {
        "id": "792b9e4b"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1008dc",
      "metadata": {
        "id": "be1008dc"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dea3272",
      "metadata": {
        "id": "5dea3272"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "dataset = load_dataset(\"KFUPM-JRCAI/arabic-generated-abstracts\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Combine all splits into one df_human\n",
        "splits = [\"by_polishing\", \"from_title\", \"from_title_and_content\"]\n",
        "\n",
        "df_human = pd.concat([dataset[s].to_pandas() for s in splits], ignore_index=True)"
      ],
      "metadata": {
        "id": "LsYtzvanJCyc"
      },
      "id": "LsYtzvanJCyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "\n",
        "for split_name in [\"by_polishing\", \"from_title\", \"from_title_and_content\"]:\n",
        "    split_df = dataset[split_name].to_pandas().copy()\n",
        "    split_df[\"source_split\"] = split_name   # <-- Create column manually\n",
        "    dfs.append(split_df)\n",
        "\n",
        "df_human = pd.concat(dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "LrKhw28dJeLz"
      },
      "id": "LrKhw28dJeLz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_rows = []\n",
        "\n",
        "for _, row in df_human.iterrows():\n",
        "    ai_models = [\n",
        "        (\"allam\", row[\"allam_generated_abstract\"]),\n",
        "        (\"jais\", row[\"jais_generated_abstract\"]),\n",
        "        (\"llama\", row[\"llama_generated_abstract\"]),\n",
        "        (\"openai\", row[\"openai_generated_abstract\"]),\n",
        "    ]\n",
        "\n",
        "    for model_name, text in ai_models:\n",
        "        ai_rows.append({\n",
        "            \"abstract_text\": text,\n",
        "            \"source_split\": row[\"source_split\"],     # now this exists\n",
        "            \"generated_by\": model_name,\n",
        "            \"label\": 0  # AI\n",
        "        })\n",
        "\n",
        "# Convert to dataframe\n",
        "df_ai = pd.DataFrame(ai_rows)\n",
        "\n",
        "# Create human dataframe\n",
        "df_h = pd.DataFrame({\n",
        "    \"abstract_text\": df_human[\"original_abstract\"],\n",
        "    \"source_split\": df_human[\"source_split\"],\n",
        "    \"generated_by\": \"human\",\n",
        "    \"label\": 1\n",
        "})\n",
        "\n",
        "# Final unified dataset\n",
        "df = pd.concat([df_h, df_ai], ignore_index=True)\n",
        "\n",
        "print(\"Final unified dataset shape:\", df.shape)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "069_1tbcJC4h"
      },
      "id": "069_1tbcJC4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xlsxwriter"
      ],
      "metadata": {
        "id": "02L0Mt0XrPj7"
      },
      "id": "02L0Mt0XrPj7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = 'original_data.xlsx'\n",
        "\n",
        "df.to_excel(\n",
        "        output_file,\n",
        "        index=False,\n",
        "        engine='xlsxwriter'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "6XlHC_iDrZQ-"
      },
      "id": "6XlHC_iDrZQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "2co4i7B7Jw8C"
      },
      "id": "2co4i7B7Jw8C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "997695cc",
      "metadata": {
        "id": "997695cc"
      },
      "source": [
        "###  Perform initial data exploration:\n",
        "\n",
        "#### 1- Load and inspect the dataset structure (columns, data types).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1452b10e",
      "metadata": {
        "id": "1452b10e"
      },
      "outputs": [],
      "source": [
        "# Inspect column names and data types for one split (e.g., 'by_polishing')\n",
        "print(\"\\nFeatures in 'by_polishing':\")\n",
        "print(dataset['by_polishing'].features)\n",
        "\n",
        "# Check dataset info (shape, structure, statistics)\n",
        "print(\"\\nDataset info for 'by_polishing':\")\n",
        "print(dataset['by_polishing'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3afb712",
      "metadata": {
        "id": "c3afb712"
      },
      "source": [
        "#### 2- Check the distribution of the target variable (label: human vs. AI- generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a289922",
      "metadata": {
        "id": "5a289922"
      },
      "outputs": [],
      "source": [
        "num_human = df[df[\"label\"] == 1].shape[0]\n",
        "num_ai = df[df[\"label\"] == 0].shape[0]\n",
        "\n",
        "total = num_human + num_ai\n",
        "\n",
        "print(\"\\n===== Target Variable Distribution =====\")\n",
        "print(\"Human-written abstracts:\", num_human)\n",
        "print(\"AI-generated abstracts:\", num_ai)\n",
        "print(\"Human %:\", round(num_human / total * 100, 2))\n",
        "print(\"AI %:\", round(num_ai / total * 100, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079c4cb2",
      "metadata": {
        "id": "079c4cb2"
      },
      "source": [
        "#### 3- Assess data quality: check for missing values, duplicates, and inconsistencies:\n",
        "\n",
        "\n",
        "Missing values → any None/NaN in columns\n",
        "\n",
        "Duplicates → same abstract appearing multiple times\n",
        "\n",
        "Inconsistencies → like empty strings \" \" or unusual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbce6d3d",
      "metadata": {
        "id": "fbce6d3d"
      },
      "outputs": [],
      "source": [
        "print(\"\\n===== Missing Values =====\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n===== Duplicate Rows =====\")\n",
        "print(\"Total duplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "print(\"\\n===== Duplicate values per column =====\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n===== Empty / Blank Values =====\")\n",
        "for col in df.columns:\n",
        "    empty_count = df[col].apply(lambda x: str(x).strip() == \"\").sum()\n",
        "    print(f\"{col}: {empty_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e825541a",
      "metadata": {
        "id": "e825541a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEON2s3Ql-tP",
      "metadata": {
        "id": "tEON2s3Ql-tP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# task 2.1: Arabic Text Preprocessing\n"
      ],
      "metadata": {
        "id": "2FTBdmjE_dVP"
      },
      "id": "2FTBdmjE_dVP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cINVvDvxo1sY",
      "metadata": {
        "id": "cINVvDvxo1sY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dge6SB2mp1kx",
      "metadata": {
        "id": "dge6SB2mp1kx"
      },
      "outputs": [],
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xTeRHx-rp43g",
      "metadata": {
        "id": "xTeRHx-rp43g"
      },
      "outputs": [],
      "source": [
        "# Check columns\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xvK4KAkWqNt-",
      "metadata": {
        "id": "xvK4KAkWqNt-"
      },
      "outputs": [],
      "source": [
        "#Define Arabic text cleaning functions\n",
        "# Remove tashkeel (diacritics)\n",
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    return re.sub(arabic_diacritics, '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v_IZB2y2q9Ip",
      "metadata": {
        "id": "v_IZB2y2q9Ip"
      },
      "outputs": [],
      "source": [
        "# Normalize Arabic text\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "    text = re.sub(\"ئ\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"[^؀-ۿ ]+\", \" \", text)  # remove non-Arabic chars\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BuV9q2b0rCtW",
      "metadata": {
        "id": "BuV9q2b0rCtW"
      },
      "outputs": [],
      "source": [
        "# Initialize stopwords and stemmer\n",
        "arabic_stopwords = set(stopwords.words(\"arabic\"))\n",
        "stemmer = ISRIStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8qhJ9wWVrFZV",
      "metadata": {
        "id": "8qhJ9wWVrFZV"
      },
      "outputs": [],
      "source": [
        "# Full preprocessing pipeline\n",
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = normalize_arabic(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in arabic_stopwords]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VDEQ2M1RrOW3",
      "metadata": {
        "id": "VDEQ2M1RrOW3"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing\n",
        "text_columns = [\n",
        "    'abstract_text',\n",
        "    'source_split',\n",
        "    'generated_by',\n",
        "    'label',\n",
        "]\n",
        "\n",
        "# Apply preprocessing on the unified abstract text column\n",
        "df[\"abstract_text_clean\"] = df[\"abstract_text\"].apply(preprocess_text)\n",
        "\n",
        "print(\"Preprocessing complete! Here are the new columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bdc3cc4",
      "metadata": {
        "id": "3bdc3cc4"
      },
      "source": [
        "\n",
        "\n",
        "## Features Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#important library\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "import regex as re2  # للاستخدام المتقدم (Arabic support)"
      ],
      "metadata": {
        "id": "fukyN6jN0tjB"
      },
      "id": "fukyN6jN0tjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions\n",
        "\n",
        "\n",
        "def simple_word_tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize text into words / symbols with Arabic support.\n",
        "    \"\"\"\n",
        "    return re2.findall(r\"\\p{Arabic}+|\\w+|[^\\s\\w]\", text, flags=re2.VERSION1)\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into sentences using Arabic/English punctuation.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    parts = re.split(r'(?<=[\\.\\?\\!\\u061F\\u061B])\\s+', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def paragraph_tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into paragraphs based on double newlines.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    paragraphs = re.split(r'\\s*\\n\\s*\\n\\s*|\\s*\\r\\n\\s*\\r\\n\\s*', text.strip())\n",
        "    return [p.strip() for p in paragraphs if p.strip()]\n"
      ],
      "metadata": {
        "id": "-wfKiITk3TRI"
      },
      "id": "-wfKiITk3TRI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column names to use\n",
        "original_text_columns = \"abstract_text\"\n",
        "clean_text_columns = \"abstract_text_clean\"\n",
        "\n",
        "\n",
        "# 1. Tokens (use clean text)\n",
        "\n",
        "df[\"tokens\"] = df[clean_text_columns].apply(\n",
        "    lambda t: [tok for tok in simple_word_tokenize(t) if tok.strip()] if isinstance(t, str) else []\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Words (use clean tokens only)\n",
        "\n",
        "df[\"words\"] = df[\"tokens\"].apply(\n",
        "    lambda toks: [tok for tok in toks if re.search(r'\\w', tok)]\n",
        ")\n",
        "\n",
        "\n",
        "# 3. Sentences (use original_text_columns for accurate sentence boundary detection)\n",
        "\n",
        "df[\"sentences\"] = df[original_text_columns].apply(\n",
        "    lambda t: sentence_tokenize(t)\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Paragraphs (use original_text_columns to preserve original structural breaks)\n",
        "\n",
        "df[\"paragraphs\"] = df[original_text_columns].apply(\n",
        "    lambda t: paragraph_tokenize(t)\n",
        ")\n",
        "\n",
        "print(\"Feature engineering completed! Columns now:\")\n",
        "print(df.columns)\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "koOJM1Pn23IR"
      },
      "id": "koOJM1Pn23IR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Xf2jiJr3jXu"
      },
      "id": "3Xf2jiJr3jXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a6bb83",
      "metadata": {
        "id": "32a6bb83"
      },
      "outputs": [],
      "source": [
        "# Column names to use\n",
        "original_text_columns = \"abstract_text\"\n",
        "clean_text_columns = \"abstract_text_clean\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fbc47e",
      "metadata": {
        "id": "68fbc47e"
      },
      "outputs": [],
      "source": [
        "# Feature 2: Number of letters / C\n",
        "import regex as _re\n",
        "\n",
        "feature = f'{original_text_columns}_f002_letters_over_C'\n",
        "\n",
        "def _ratio_letters(t):\n",
        "    s = str(t) if pd.notna(t) else \"\"\n",
        "    C = len(s)\n",
        "    if C == 0:\n",
        "        return 0.0\n",
        "    letters = len(_re.findall(r'\\p{L}', s, flags=_re.VERSION1))\n",
        "    return letters / C\n",
        "\n",
        "df[feature] = df[original_text_columns].apply(_ratio_letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097d788f",
      "metadata": {
        "id": "097d788f"
      },
      "outputs": [],
      "source": [
        "# Feature 21: Brunet's W measure (approx)\n",
        "#Brunet's W هو مقياس لتنوع المفردات في النص.\n",
        "#قيمة W أقل → النص أكثر تنوعًا.\n",
        "def _brunet_W(words, alpha=0.172):\n",
        "    \"\"\"Calculates Brunet's W measure of lexical diversity.\"\"\"\n",
        "    N = len(words) # Total tokens\n",
        "    freq = Counter([w.lower() for w in words])\n",
        "    V = len(freq) # Total types\n",
        "\n",
        "    if N > 0 and V > 0:\n",
        "        try:\n",
        "            # W = N ^ (V ^ (-alpha))\n",
        "            return N ** (V ** (-alpha))\n",
        "        except OverflowError: # Handle potential large number errors gracefully\n",
        "            return 0.0\n",
        "    return 0.0\n",
        "\n",
        "feature_name = f'{clean_text_columns}_f021_brunet_W'\n",
        "df[feature_name] = df[\"words\"].apply(_brunet_W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6f1f14",
      "metadata": {
        "id": "7b6f1f14"
      },
      "outputs": [],
      "source": [
        "# (40) Sentences length frequency distribution\n",
        "df['f040_Sentence_length_frequency_distribution'] = df[\"sentences\"].apply(\n",
        "    # Assuming simple_word_tokenize is available\n",
        "    lambda s: dict(Counter([len(simple_word_tokenize(sent)) for sent in s]))\n",
        "    if s else {}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyarabic"
      ],
      "metadata": {
        "id": "B_cESchKux0Y"
      },
      "id": "B_cESchKux0Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as _re          # better than \"re\" for Arabic letters\n",
        "import pyarabic.araby as ar  # optional: Arabic normalization & letter handling"
      ],
      "metadata": {
        "id": "q12f-7eb5wMD"
      },
      "id": "q12f-7eb5wMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace tokenizer for embeddings\n",
        "from transformers import AutoTokenizer\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Us1sMLAdvW6x"
      },
      "id": "Us1sMLAdvW6x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#59.Number of words found in the 500 positions within word  embedding (use corresponding word embedding aligned with the used LLM model)\n",
        "#حساب أعلى 500 كلمة لكل مودل مسستخدم بالداتا\n",
        "\n",
        "\n",
        "#  Load tokenizer (aligned to LLM)\n",
        "\n",
        "model_name = \"bert-base-uncased\"   # change to your embedding model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Extract top 500 vocabulary tokens\n",
        "top_500_vocab = set(list(tokenizer.get_vocab().keys())[:500])\n",
        "\n",
        "\n",
        "#  Feature 3: Words found in top-500 embedding positions\n",
        "\n",
        "feature = f\"{clean_text_columns}_f059_words_in_top500\"\n",
        "\n",
        "def _count_words_in_top500(text):\n",
        "    if text is None:\n",
        "        return 0\n",
        "\n",
        "    # Convert to string + normalize Arabic\n",
        "    s = str(text)\n",
        "    s = ar.normalize_hamza(s)             # normalize different hamza forms\n",
        "    s = ar.normalize_ligature(s)          # normalize Arabic ligatures\n",
        "    s = ar.strip_tashkeel(s)              # remove diacritics\n",
        "\n",
        "    # split words on Arabic/English boundaries\n",
        "    words = _re.findall(r\"\\p{L}+\", s, flags=_re.VERSION1)\n",
        "\n",
        "    # count matches with top 500 tokenizer words\n",
        "    return sum(1 for w in words if w.lower() in top_500_vocab)\n",
        "\n",
        "# Apply feature\n",
        "df[feature] = df[clean_text_columns].apply(_count_words_in_top500)"
      ],
      "metadata": {
        "id": "rNGZShfWM2JW"
      },
      "id": "rNGZShfWM2JW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "ocUyT3hmYHC5"
      },
      "id": "ocUyT3hmYHC5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  لتطبيق الميزة 78\n",
        "!pip install transformers torch\n"
      ],
      "metadata": {
        "id": "yAR9DXOQFerA"
      },
      "id": "yAR9DXOQFerA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc35514",
      "metadata": {
        "id": "1dc35514"
      },
      "outputs": [],
      "source": [
        "# Feature 78: Perplexity score (placeholder). Requires LM scoring. Set up transformers and compute if desired.\n",
        "#ميزة تعتمد على LM scoring، أي استخدام نموذج لغة (مثل GPT أو أي نموذج من مكتبة Hugging Face transformers) لحساب perplexity للنصوص. الـ perplexity هو مقياس لمدى قدرة النموذج على التنبؤ بالنص: كلما كان الرقم أصغر، كان النص أكثر “تناسقًا” أو متوافقًا مع اللغة التي تعلم عليها النموذج.\n",
        "#مكتبات لازمة\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_name_ppx = \"aubmindlab/aragpt2-base\"   # Arabic GPT2 model\n",
        "tokenizer_ppx = AutoTokenizer.from_pretrained(model_name_ppx)\n",
        "model_ppx = AutoModelForCausalLM.from_pretrained(model_name_ppx)\n",
        "model_ppx.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ppx.to(device)\n",
        "\n",
        "# Feature 78 name\n",
        "feature_ppx = f\"{clean_text_columns}_f078_perplexity\"\n",
        "\n",
        "def _calculate_perplexity_ar(text):\n",
        "    text = str(text).strip()\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer_ppx(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model_ppx(input_ids, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return float(torch.exp(loss).cpu().item())\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "  #Apply Feature 78 to the clean text column\n",
        "df[feature_ppx] = df[clean_text_columns].apply(\n",
        "    lambda t: _calculate_perplexity_ar(t) if pd.notna(t) else None\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = 'clean_data.xlsx'\n",
        "\n",
        "df.to_excel(\n",
        "        output_file,\n",
        "        index=False,\n",
        "        engine='xlsxwriter'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "J54js1KsxO8r"
      },
      "id": "J54js1KsxO8r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the data\n",
        "\n"
      ],
      "metadata": {
        "id": "UI6d-tjLpHV6"
      },
      "id": "UI6d-tjLpHV6"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3S7SrW6xOHa"
      },
      "id": "O3S7SrW6xOHa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: Train 70%, Temp 30%\n",
        "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, shuffle=True)\n",
        "\n",
        "# Second split: Temp 30% → 15% Validation, 15% Test\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42, shuffle=True)\n",
        "\n",
        "# Show sizes\n",
        "print(\"TOTAL:\", len(df))\n",
        "print(\"TRAIN:\", len(train_df))\n",
        "print(\"VAL:\", len(val_df))\n",
        "print(\"TEST:\", len(test_df))"
      ],
      "metadata": {
        "id": "z5KRZ435mSIT"
      },
      "id": "z5KRZ435mSIT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF Features from Cleaned Text\n"
      ],
      "metadata": {
        "id": "X8EmxCBq1svO"
      },
      "id": "X8EmxCBq1svO"
    },
    {
      "cell_type": "code",
      "source": [
        "#apply with abstract_text_clean only\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer for Arabic text\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,   # limit vocabulary\n",
        "    ngram_range=(1,2),   # unigrams + bigrams\n",
        "    analyzer='word'\n",
        ")\n",
        "\n",
        "# Fit only on training set\n",
        "tfidf_vectorizer.fit(train_df[\"abstract_text_clean\"])\n",
        "\n",
        "# Transform train/validation/test sets\n",
        "X_train_tfidf = tfidf_vectorizer.transform(train_df[\"abstract_text_clean\"])\n",
        "X_val_tfidf   = tfidf_vectorizer.transform(val_df[\"abstract_text_clean\"])\n",
        "X_test_tfidf  = tfidf_vectorizer.transform(test_df[\"abstract_text_clean\"])\n",
        "\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"Train:\", X_train_tfidf.shape)\n",
        "print(\"Validation:\", X_val_tfidf.shape)\n",
        "print(\"Test:\", X_test_tfidf.shape)"
      ],
      "metadata": {
        "id": "jG6UsoqfxcAp"
      },
      "id": "jG6UsoqfxcAp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define X and y"
      ],
      "metadata": {
        "id": "LjAV35ij8KUf"
      },
      "id": "LjAV35ij8KUf"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack\n"
      ],
      "metadata": {
        "id": "dcwV8k-FBGRa"
      },
      "id": "dcwV8k-FBGRa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select numeric features (The generated feature engineering exclude label and text)\n",
        "EXCLUDED_COLS = ['label', 'abstract_text', 'abstract_text_clean',\n",
        "                 'tokens', 'words', 'sentences', 'paragraphs', 'abstract_text_pos_tags']\n",
        "# Select columns that are numeric AND not in the exclusion list>>feature engineering columns\n",
        "numeric_cols = [\n",
        "    col for col in train_df.select_dtypes(include=np.number).columns.tolist()\n",
        "    if col not in EXCLUDED_COLS\n",
        "]\n",
        "# Convert the numeric features DataFrames to NumPy arrays (dense matrices)\n",
        "# We must use the values/to_numpy() method to extract the array for sparse matrix stacking.\n",
        "X_train_num_array = train_df[numeric_cols].values\n",
        "X_val_num_array   = val_df[numeric_cols].values\n",
        "X_test_num_array  = test_df[numeric_cols].values\n",
        "\n",
        "\n",
        "# Target variable\n",
        "y_train = train_df[\"label\"]\n",
        "y_val   = val_df[\"label\"]\n",
        "y_test  = test_df[\"label\"]\n",
        "\n",
        "# Features: TF-IDF and the creating feature engineering\n",
        "X_train = hstack([X_train_tfidf, X_train_num_array]).tocsr()\n",
        "X_val= hstack([X_val_tfidf, X_val_num_array]).tocsr()\n",
        "X_test= hstack([X_test_tfidf, X_test_num_array]).tocsr()\n",
        "\n",
        "print(\"X and y are ready for ML models.\")\n",
        "print(\"Train:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation:\", X_val.shape, y_val.shape)\n",
        "print(\"Test:\", X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "-5zn9gPb8P_u"
      },
      "id": "-5zn9gPb8P_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Machine learning Models"
      ],
      "metadata": {
        "id": "SE-PVHjRxgt2"
      },
      "id": "SE-PVHjRxgt2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1-Baseline Model (Naïve Bayes & Logistic Regression)"
      ],
      "metadata": {
        "id": "6oSy1mpLycxk"
      },
      "id": "6oSy1mpLycxk"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the model\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train on training set\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = lr_model.predict(X_val)\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "Br5vpRFdyb0F"
      },
      "id": "Br5vpRFdyb0F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# Predict on test set\n",
        "y_test_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Optional: confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ty8EIq-ExzP1"
      },
      "id": "ty8EIq-ExzP1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-Traditional Machine Learning Models ( Support Vector Machine (SVM), Random Forest, XGBoost)using the validation set"
      ],
      "metadata": {
        "id": "6NKTL3FCyuh2"
      },
      "id": "6NKTL3FCyuh2"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# إذا كانت X_train sparse (مثل TF-IDF)، لا نستخدم StandardScaler مع with_mean=True\n",
        "n_components = 300  # يمكن تعديلها\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "\n",
        "svm_pipeline = Pipeline([\n",
        "    ('svd', svd),\n",
        "    ('svm', LinearSVC(C=1.0, max_iter=10000, random_state=42))\n",
        "])\n",
        "\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "y_val_pred_svm = svm_pipeline.predict(X_val)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Linear SVM Validation Accuracy:\", accuracy_score(y_val, y_val_pred_svm))\n",
        "print(classification_report(y_val, y_val_pred_svm))\n"
      ],
      "metadata": {
        "id": "hlje_XmCxzTu"
      },
      "id": "hlje_XmCxzTu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# XGBoost محسّن للبيانات الكبيرة / sparse\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,      # عدد الأشجار\n",
        "    max_depth=6,           # أقصى عمق لكل شجرة\n",
        "    learning_rate=0.1,     # سرعة التعلم\n",
        "    subsample=0.8,         # أخذ عينات من البيانات لتسريع التدريب وتجنب overfitting\n",
        "    colsample_bytree=0.8,  # أخذ عينات من المميزات لكل شجرة\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    n_jobs=-1,             # استخدام كل الأنوية المتاحة\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# تدريب النموذج\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# التنبؤ والتحقق من الأداء\n",
        "y_val_pred_xgb = xgb_model.predict(X_val)\n",
        "print(\"XGBoost Validation Accuracy:\", accuracy_score(y_val, y_val_pred_xgb))\n",
        "print(classification_report(y_val, y_val_pred_xgb))"
      ],
      "metadata": {
        "id": "stfT1bWmJbSe"
      },
      "id": "stfT1bWmJbSe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# حفظ النماذج في قاموس لتسهيل التقييم\n",
        "models = {\n",
        "    'LinearSVM': svm_pipeline,\n",
        "    'XGBoost': xgb_model\n",
        "}\n",
        "\n",
        "# تقييم كل نموذج على مجموعة الاختبار\n",
        "for name, model in models.items():\n",
        "    # التنبؤ على مجموعة الاختبار\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # طباعة النتائج\n",
        "    print(f\"\\n===== {name} Test Evaluation =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # مصفوفة الالتباس\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "                xticklabels=True, yticklabels=True)\n",
        "    plt.title(f'Confusion Matrix - {name}', fontsize=14)\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('Actual', fontsize=12)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "u7r1W70PFGCI"
      },
      "id": "u7r1W70PFGCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Deep learning Models"
      ],
      "metadata": {
        "id": "XynuPVgkxpx7"
      },
      "id": "XynuPVgkxpx7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build neural network classifier ( simple Feedforward Network) on top of the extracted BERT embeddings or fine-tune the pre-trained BERT model for the classification task."
      ],
      "metadata": {
        "id": "zI-bNgZ-zYYP"
      },
      "id": "zI-bNgZ-zYYP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step1: Extract BERT Embeddings (Sentence-level)"
      ],
      "metadata": {
        "id": "bk5fAQDxFasf"
      },
      "id": "bk5fAQDxFasf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "qqZOePowtg66"
      },
      "id": "qqZOePowtg66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CPj82dOOVgeE"
      },
      "id": "CPj82dOOVgeE"
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running embeddings on:\", device)\n",
        "\n",
        "# تحميل نموذج صغير نسبيًا لتسريع الـ embeddings على CPU\n",
        "bert_model = SentenceTransformer(\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# الأعمدة المراد استبعادها\n",
        "exclude_cols = [\"label\", \"abstract_text\"]\n",
        "\n",
        "# دالة لدمج كل الأعمدة النصية المتبقية في عمود واحد\n",
        "def combine_text_columns(df, exclude_cols):\n",
        "    text_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "    combined_texts = df[text_cols].astype(str).agg(\" \".join, axis=1).tolist()\n",
        "    return combined_texts\n",
        "\n",
        "# إعداد tokenizer لاختصار النصوص إذا كانت طويلة جدًا\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "max_tokens = 128  # عدد التوكن لكل نص لتسريع الحساب على CPU\n",
        "\n",
        "def truncate_texts(texts, max_length=max_tokens):\n",
        "    return [\" \".join(tokenizer.tokenize(t)[:max_length]) for t in texts]\n",
        "\n",
        "# --- تحضير البيانات ---\n",
        "train_texts = truncate_texts(combine_text_columns(train_df, exclude_cols))\n",
        "val_texts   = truncate_texts(combine_text_columns(val_df, exclude_cols))\n",
        "test_texts  = truncate_texts(combine_text_columns(test_df, exclude_cols))\n",
        "\n",
        "# --- دالة لتطبيق embeddings على دفعات لتسريع CPU ---\n",
        "def encode_in_batches(texts, model, batch_size=128):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "        embeddings.append(emb)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# --- إنشاء embeddings ---\n",
        "X_train_emb = encode_in_batches(train_texts, bert_model, batch_size=128)\n",
        "X_val_emb   = encode_in_batches(val_texts, bert_model, batch_size=128)\n",
        "X_test_emb  = encode_in_batches(test_texts, bert_model, batch_size=128)\n",
        "\n",
        "# --- المتغيرات المستهدفة ---\n",
        "y_train = train_df[\"label\"].values\n",
        "y_val   = val_df[\"label\"].values\n",
        "y_test  = test_df[\"label\"].values\n",
        "\n",
        "# --- طباعة الأبعاد للتأكد ---\n",
        "print(\"Train embedding shape:\", X_train_emb.shape)\n",
        "print(\"Validation embedding shape:\", X_val_emb.shape)\n",
        "print(\"Test embedding shape:\", X_test_emb.shape)\n"
      ],
      "metadata": {
        "id": "BEwOGemwQzAF"
      },
      "id": "BEwOGemwQzAF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Build a Feedforward Neural Network"
      ],
      "metadata": {
        "id": "xuUDRJAdFgCI"
      },
      "id": "xuUDRJAdFgCI"
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Basic feedforward classifier on embeddings\n",
        "ffnn_model = models.Sequential([\n",
        "    layers.Input(shape=(X_train_emb.shape[1],)),\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation=\"sigmoid\")   # binary classification\n",
        "])\n",
        "\n",
        "ffnn_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "ffnn_model.summary()"
      ],
      "metadata": {
        "id": "SpSCctGJFTjh"
      },
      "id": "SpSCctGJFTjh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step3: Train the Model"
      ],
      "metadata": {
        "id": "PlpuAuhcFktY"
      },
      "id": "PlpuAuhcFktY"
    },
    {
      "cell_type": "code",
      "source": [
        "history = ffnn_model.fit(\n",
        "    X_train_emb, y_train,\n",
        "    validation_data=(X_val_emb, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "x5jMscMdFTxu"
      },
      "id": "x5jMscMdFTxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Evaluate on Test Set"
      ],
      "metadata": {
        "id": "GdNX9LG7FrqY"
      },
      "id": "GdNX9LG7FrqY"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict\n",
        "y_test_pred = (ffnn_model.predict(X_test_emb) > 0.5).astype(int)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "inApdWeOFT5k"
      },
      "id": "inApdWeOFT5k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save Models"
      ],
      "metadata": {
        "id": "vdpDnH4AFwcI"
      },
      "id": "vdpDnH4AFwcI"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import Model as KerasModel\n",
        "\n",
        "def save_all_models(models_dict, save_dir=\"models\"):\n",
        "\n",
        "\n",
        "    # Create save folder\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for model_name, model_obj in models_dict.items():\n",
        "\n",
        "\n",
        "            file_path = os.path.join(save_dir, f\"{model_name}.pkl\")\n",
        "            joblib.dump(model_obj, file_path)\n",
        "            print(f\"[Saved] Pickle model → {file_path}\")\n",
        "\n",
        "    print(\"\\nAll models saved successfully!\")"
      ],
      "metadata": {
        "id": "Hiy3g22LFzdZ"
      },
      "id": "Hiy3g22LFzdZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "models_dict = {\n",
        "    \"lr_model\": lr_model,\n",
        "    \"svm\": svm_pipeline,\n",
        "    \"xgboost\": xgb_model,\n",
        "    \"ffnn\": ffnn_model\n",
        "}\n",
        "\n",
        "save_all_models(models_dict)"
      ],
      "metadata": {
        "id": "Q0sivqoiF37M"
      },
      "id": "Q0sivqoiF37M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "oXMaomKhn84U"
      },
      "id": "oXMaomKhn84U",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}